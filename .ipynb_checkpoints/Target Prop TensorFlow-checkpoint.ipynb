{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target prop in a deep linear network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reset -f\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rank_one_matrix():\n",
    "    ''' return: random 2x2 rank one matrix'''\n",
    "    W_ = np.random.randn(2,2)\n",
    "    u,s,v = np.linalg.svd(W_)\n",
    "    return np.outer(u[:,0],v[0])\n",
    "\n",
    "def simple_matrix():\n",
    "    ''' return: a specific 2x2 rank one matrix '''\n",
    "    return np.outer(np.array([1,0]), np.array([1,2])/np.linalg.norm([1,2]))\n",
    "\n",
    "def get_subspaces(A_, r):\n",
    "    ''' return: a dict containing the four fundamental subspaces of a matrix A '''\n",
    "    u,s,v = np.linalg.svd(A_, full_matrices=True)\n",
    "    ss = {'im': u[:,:r], 'ker': v[r:].T, 'coim': v[:r].T, 'coker': u[:,r:]}\n",
    "    return ss\n",
    "\n",
    "def plot_vecline(V, i=0, color=0):\n",
    "    ''' plots the 1d subspace (line) spanned by one vector V '''\n",
    "    VV = np.concatenate((-10*V, 10*V), axis=1)\n",
    "    ax[i].plot(VV[0], VV[1], color=sns.color_palette(\"RdBu_r\",7)[color], linewidth=2.0)\n",
    "    \n",
    "def plot_quotient(V, W, i=0, color=5):\n",
    "    ''' plots elements (lines) of the quotient space V/W \n",
    "        todo: replace n with sv or 1/sv\n",
    "    '''\n",
    "    for n in np.arange(-3,3.1,0.5):\n",
    "        VV = np.concatenate((-10*V, 10*V), axis=1)\n",
    "        VV = VV + n*W\n",
    "        ax[i].plot(VV[0], VV[1], color=sns.color_palette(\"RdBu_r\",7)[color], linewidth=0.5)\n",
    "\n",
    "def square_axes(i=0):\n",
    "    ''' make axis look nice '''\n",
    "    fig.axes[i].axhline(0, color='w', linewidth=3.5)\n",
    "    fig.axes[i].axvline(0, color='w', linewidth=3.5)\n",
    "    fig.axes[i].set_xlim(-2,2)\n",
    "    fig.axes[i].set_ylim(-2,2)\n",
    "    fig.axes[i].set_aspect(1)\n",
    "    fig.axes[i].get_xaxis().set_ticklabels([])\n",
    "    fig.axes[i].get_yaxis().set_ticklabels([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Forward activation\n",
    "def forward(x_init, W_in):\n",
    "    ''' get activations for all neurons '''\n",
    "    # TODO: show two+ vectors / batch learning.\n",
    "    # batch learning: first then second vs second then first vs batch -- commutivity? \n",
    "    x_ = []\n",
    "    x_.append(x_init)\n",
    "    for l in range(layers):\n",
    "        x_.append(np.dot(W_in[l], x_[-1]))\n",
    "    return x_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Backprop\n",
    "def backward(x_end, W_in):\n",
    "    ''' get dL/dx[l] for all x '''\n",
    "    dL_ = (layers+1)*[None]\n",
    "    dL_[-1] = (x_end-y) # negative gradient of MSE\n",
    "    for l in range(layers-1,-1,-1):\n",
    "        dL_[l] = np.dot(W_in[l].T, dL_[l+1])\n",
    "    return dL_\n",
    "\n",
    "def train_weights(W_in, dL_in, x_in, alpha=.1):\n",
    "    ''' get dL/dW[l] for all l '''\n",
    "    W_ = np.copy(W_in)\n",
    "    for l in range(layers):\n",
    "        W_[l] = W_[l] - alpha*np.outer(dL_in[l+1], x_in[l])\n",
    "    return W_\n",
    "\n",
    "def get_W_update(dL_t_in, x_t_in):\n",
    "    ''' get np.outer(dL[l+1], x[l]), the rank-one update for the weights in backprop. '''\n",
    "    W_up = train_steps*[layers*[None]]\n",
    "    W_up_ss = train_steps*[layers*[None]]\n",
    "    for i in range(train_steps):\n",
    "        for l in range(layers):\n",
    "            W_up[i][l] = np.outer(dL_t_in[i][l+1], x_t_in[i][l])\n",
    "            W_up_ss[i][l] = get_subspaces(W_up[i][l], 1)\n",
    "    return W_up, W_up_ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Target prop\n",
    "def get_pinv(W_in):\n",
    "    ''' get pinv '''\n",
    "    W_pinv_ = layers*[None]\n",
    "    for l in range(layers):\n",
    "        W_pinv_[l] = np.linalg.pinv(W_in[l])\n",
    "    return W_pinv_\n",
    "\n",
    "def get_targets(x_in, W_in, dL_in):\n",
    "    '''  '''\n",
    "    x_tar_ = (layers+1)*[None]\n",
    "    W_pinv_ = get_pinv(W_in)\n",
    "    x_tar_[-1] = x_in[-1] - alpha*dL_in[-1]\n",
    "    for l in range(layers-1,-1,-1):\n",
    "        x_tar_[l] = x_in[l] - np.dot(W_pinv_[l], x_in[l+1]) + np.dot(W_pinv_[l],x_tar_[l+1])\n",
    "    return x_tar_\n",
    "\n",
    "def train_weights_targ(W_in, x_targ_in, x_in, alpha=.1):\n",
    "    ''' train weights, targ prop version '''\n",
    "    dL_local = [x_in[l] - x_targ_in[l] for l in range(len(x_in))]\n",
    "    W_ = train_weights(W_in, dL_local, x_in, alpha=alpha)\n",
    "    return W_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# construct model:\n",
    "layers = 2\n",
    "W = []\n",
    "W_ss = []\n",
    "\n",
    "train_steps = 0\n",
    "alpha = .3\n",
    "\n",
    "for l in range(layers):\n",
    "    W.append(simple_matrix())\n",
    "    W_ss.append(get_subspaces(W[l], 1))\n",
    "\n",
    "# Data\n",
    "x_0 = np.array([-1.5,-1.15])\n",
    "y = np.array([1,0.5])\n",
    "    \n",
    "# Cost function visualization\n",
    "vv, hh = np.mgrid[-2:2:20j, -2:2:20j]\n",
    "L = 0.5*((hh-y[0])**2 + (vv-y[1])**2)\n",
    "\n",
    "# First pass\n",
    "x = forward(x_0, W)\n",
    "dL = backward(x[-1], W)\n",
    "x_tar = get_targets(x, W, dL)\n",
    "\n",
    "# Training steps\n",
    "x_bp_t = [x]\n",
    "W_bp_t = [W]\n",
    "dL_t = [dL]\n",
    "\n",
    "x_tp_t = [x]\n",
    "x_tar_t = [x_tar]\n",
    "W_tp_t = [W]\n",
    "\n",
    "# Training\n",
    "for i in range(train_steps):\n",
    "    # backprop\n",
    "    W_bp_t.append(train_weights(W_bp_t[-1], dL_t[-1], x_bp_t[-1], alpha=alpha))\n",
    "    x_bp_t.append(forward(x_0, W_bp_t[i]))\n",
    "    dL_t.append(backward(x_bp_t[-1][-1], W_bp_t[-1]))\n",
    "    \n",
    "    # target prop\n",
    "    W_tp_t.append(train_weights_targ(W_tp_t[-1], x_tar_t[-1], x_tp_t[-1], alpha=alpha))\n",
    "    x_tp_t.append(forward(x_0, W_tp_t[i]))\n",
    "    x_tar_t.append(get_targets(x_tp_t[-1], W_tp_t[-1], dL_t[-1]))\n",
    "\n",
    "# pseudoinv\n",
    "W_pi = []\n",
    "W_ps = []\n",
    "for i in range(train_steps+1):\n",
    "    W_pi.append(get_pinv(W_tp_t[i]))\n",
    "for l in range(layers):\n",
    "    W_ps.append(get_subspaces(W_pi[0][l], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_dots(ax_):\n",
    "    for l in range(layers+1):\n",
    "        for t in [c+1 for c in range(train_steps-2,-1,-1)]:\n",
    "            #ax_[l].plot(x_bp_t[t+1][l][0], x_bp_t[t+1][l][1], 'o', color=sns.color_palette()[0], ms=10, alpha=0.1)\n",
    "            #ax_[l].plot(dL_t[t+1][l][0], dL_t[t+1][l][1], 'o', color=sns.color_palette()[1], ms=10, alpha=0.1)\n",
    "            ax_[l].plot(x_tp_t[t+1][l][0], x_tp_t[t+1][l][1], '*', color=sns.color_palette()[0], ms=10, alpha=0.5)\n",
    "            ax_[l].plot(x_tar_t[t+1][l][0], x_tar_t[t+1][l][1], '*', color=sns.color_palette()[1], ms=10, alpha=0.5)\n",
    "\n",
    "        #ax_[l].plot(x_bp_t[0][l][0], x_bp_t[0][l][1], 'o', color=sns.color_palette()[0], ms=10)\n",
    "        #ax_[l].plot(dL_t[0][l][0], dL_t[0][l][1], 'o', color=sns.color_palette()[1], ms=10)\n",
    "        ax_[l].plot(x_tp_t[0][l][0], x_tp_t[0][l][1], '*', color=sns.color_palette()[0], ms=10)\n",
    "        ax_[l].plot(x_tar_t[0][l][0], x_tar_t[0][l][1], '*', color=sns.color_palette()[1], ms=10)\n",
    "    ax_[-1].plot(y[0],y[1], '^', color=sns.color_palette()[4], ms=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## ROW 1\n",
    "fig, ax = plt.subplots(1, layers+1, figsize=(20,20))\n",
    "for l in range(layers+1):\n",
    "    square_axes(l)\n",
    "\n",
    "for l in range(layers):\n",
    "    plot_quotient(W_ss[l]['ker'], W_ss[l]['coim'], l)\n",
    "    plot_vecline(W_ss[l]['ker'], l, -1)\n",
    "    plot_vecline(W_ss[l]['coim'],l, 1)\n",
    "\n",
    "plot_dots(ax)\n",
    "ax[-1].contour(hh, vv, L, 10)\n",
    "plt.show()\n",
    "\n",
    "## ROW 2\n",
    "fig, ax = plt.subplots(1, layers+1, figsize=(20,20))\n",
    "for l in range(layers+1):\n",
    "    square_axes(l)\n",
    "\n",
    "for l in range(layers):\n",
    "    plot_quotient(W_ss[l]['coker'], W_ss[l]['im'], l+1)\n",
    "    plot_vecline(W_ss[l]['coker'], l+1, -1)\n",
    "    plot_vecline(W_ss[l]['im'],l+1, 1)\n",
    "\n",
    "plot_dots(ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- W[l] moves in the direction to minimize error.\n",
    "- Does this imply that x[l] also moves in the direction to minimize error?\n",
    "\n",
    "\n",
    "- Specify the space of possible solutions -- global sections of x[l], but also space of W[l]?\n",
    "\n",
    "\n",
    "- visualize the gradient functionals \n",
    "\n",
    "\n",
    "- Understand why for backprop:\n",
    "    - dL/dW does not change over training steps (uh, maybe a bug?)\n",
    "    - dL/dW is specifically rank-one. What is gained by higher-rank updates. Relation to FORCE / RLS / Full-FORCE. Newton's method -> higher thank rank-one?\n",
    "    \n",
    "\n",
    "- should be able to prove why W_update is constant for deep linear networks. Implications?\n",
    "\n",
    "\n",
    "- confirm: does x[t] + alpha*dL/dx[t] = x[t] + alpha*(x_prevlayer[t])\n",
    "\n",
    "dL/dx[l] says: move in this direction to reduce error. And dL/dW[l] says: \n",
    "\n",
    "\n",
    "- idea: plot loss contours at each layer -- i.e. backprop the entire error function... (?)\n",
    "\n",
    "\n",
    "- more generally: play with visualizations for images, kernel, preimage, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_array = np.array(x_tp_t)\n",
    "dL_array = np.array(dL_t)\n",
    "W_array = np.array(W_tp_t)\n",
    "#W_update_array = np.array(W_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, layers+1, figsize=(20,4))\n",
    "for l in range(layers+1):\n",
    "    ax[l].plot(x_array[:,l,:])\n",
    "    ax[l].plot(dL_array[:,l,:],'--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, layers, figsize=(20,4))\n",
    "for l in range(layers):\n",
    "    ax[l].plot(W_array[:,l,:,0])\n",
    "    ax[l].plot(W_array[:,l,:,1])\n",
    "    ax[l].plot(W_update_array[:,l,:,0],'--')\n",
    "    ax[l].plot(W_update_array[:,l,:,1],'--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W_update_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tr = 30\n",
    "print x_t[tr][2]\n",
    "print y\n",
    "print x_t[tr][2] - y\n",
    "print x_t[tr][1]\n",
    "\n",
    "print np.outer(x_t[tr][2]-y, x_t[tr][1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target prop in tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reset -f\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Toy data.\n",
    "vv, hh = np.mgrid[-2:2:50j, -2:2:50j]\n",
    "\n",
    "x_data = np.stack((vv.ravel(), hh.ravel()), axis=1)\n",
    "\n",
    "# y data\n",
    "rad = np.linalg.norm(x_data, axis=1)\n",
    "y_data = 2*np.logical_and(rad>0.8, rad<1.2).astype('float')-1\n",
    "y_data = np.stack((y_data, np.zeros(y_data.shape)), axis=1)\n",
    "\n",
    "# params\n",
    "m_dim = 2\n",
    "p_dim = 2\n",
    "batch_size = 100\n",
    "num_samples = y_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_rand_batch(data):\n",
    "    inds = np.random.choice(num_samples, batch_size)\n",
    "    for i in range(len(data)):\n",
    "        data[i] = data[i][inds]\n",
    "    return inds, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "### Mnist data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow.examples.tutorials.mnist import mnist\n",
    "\n",
    "mnist_data = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "num_samples = mnist_data.train.num_examples\n",
    "m_dim = mnist.IMAGE_PIXELS\n",
    "p_dim = mnist.NUM_CLASSES\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_data = mnist_data.train.images\n",
    "y_data = mnist_data.train.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Model\n",
    "tf.reset_default_graph()\n",
    "\n",
    "layers = 5\n",
    "l_dim = [m_dim] + (layers-1)*[100] + [p_dim]\n",
    "stddev = 0.01\n",
    "b_init = 0.1\n",
    "\n",
    "alpha = 1\n",
    "\n",
    "# Placeholders\n",
    "x_in = tf.placeholder(tf.float32, shape=[batch_size, m_dim], name='x0')\n",
    "y = tf.placeholder(tf.float32, shape=[batch_size, p_dim], name='y')\n",
    "epoch = tf.placeholder(tf.float32, shape=None, name='std')\n",
    "\n",
    "noise_inj = 10/(1+epoch/100)\n",
    "\n",
    "# Initialize lists\n",
    "b = (layers+1)*[None]\n",
    "W = (layers+1)*[None]\n",
    "x = (layers+1)*[None]\n",
    "x[0] = x_in\n",
    "\n",
    "x_tar = (layers+1)*[None]\n",
    "V = (layers+1)*[None]\n",
    "c = (layers+1)*[None]\n",
    "\n",
    "gx = (layers+1)*[None]\n",
    "gx_tar = (layers+1)*[None]\n",
    "\n",
    "L = (layers+1)*[None]\n",
    "L_inv = (layers+1)*[None]\n",
    "\n",
    "# Forward graph\n",
    "for l in range(1, layers+1):\n",
    "    with tf.name_scope('layer'+str(l)) as scope:\n",
    "        b[l] = tf.Variable(tf.constant(b_init, shape=[1, l_dim[l]]), name='b_'+str(l))\n",
    "        W[l] = tf.Variable(tf.truncated_normal([l_dim[l-1], l_dim[l]], stddev=stddev), name='W_'+str(l))\n",
    "        x[l] = tf.add(tf.matmul(x[l-1], W[l]), b[l], name='x_'+str(l)) # better way to name?\n",
    "\n",
    "# Global loss\n",
    "#L_g = tf.reduce_mean(0.5*(x[-1] - y)**2, name=\"global_loss\")\n",
    "L_g = tf.reduce_mean(-tf.reduce_sum(y*tf.log(tf.nn.softmax(x[-1])), reduction_indices=[1]), name=\"global_loss\")\n",
    "\n",
    "# Top-layer targets\n",
    "x_tar[-1] = x[-1] - alpha*tf.gradients(L_g, x[-1])[0]\n",
    "x_tar[-2] = x[-2] - alpha*tf.gradients(L_g, x[-2])[0] # tf.gradients can't go backward through the graph... \n",
    "\n",
    "# Target graph\n",
    "for l in range(layers,1,-1): # from M to 2\n",
    "    with tf.name_scope('layer_target'+str(l)) as scope:\n",
    "        c[l] = tf.Variable(tf.constant(b_init, shape=[1, l_dim[l-1]]), name='c_'+str(l))\n",
    "        #c[l] = tf.Variable(tf.constant(b_init, shape=[1, l_dim[l-1]]), name='c_'+str(l))\n",
    "        V[l] = tf.Variable(tf.truncated_normal([l_dim[l], l_dim[l-1]], stddev=stddev), name='V_'+str(l))\n",
    "        gx[l] = tf.nn.sigmoid(tf.matmul(x[l], V[l]) + c[l], name='g_x_'+str(l))\n",
    "        gx_tar[l] = tf.nn.sigmoid(tf.matmul(x_tar[l], V[l]) + c[l], name='g_x_tar'+str(l))\n",
    "        if x_tar[l-1] is None:\n",
    "            x_tar[l-1] = x[l-1] + gx_tar[l] - gx[l] # give name\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(1), Dimension(10)])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "opt = tf.train.GradientDescentOptimizer(0.5)\n",
    "\n",
    "train_op_inv = (layers+1)*[None]\n",
    "train_op = (layers+1)*[None]\n",
    "    \n",
    "fx_ = (layers+1)*[None]\n",
    "gx_ = (layers+1)*[None]\n",
    "\n",
    "for l in range(1, layers+1):\n",
    "    with tf.name_scope('layer'+str(l)) as scope:\n",
    "        L[l] = tf.reduce_mean(0.5*(x[l] - x_tar[l])**2, name=\"L\"+str(l))\n",
    "        train_op[l] = opt.minimize(L[l], var_list=[W[l], b[l]])\n",
    "\n",
    "for l in range(2, layers+1):\n",
    "    with tf.name_scope('layer'+str(l)) as scope:\n",
    "        fx_[l] = tf.nn.sigmoid(tf.matmul(x[l-1], W[l]) + b[l] + tf.random_normal(b[l].get_shape(), stddev=noise_inj))\n",
    "        gx_[l] = tf.nn.sigmoid(tf.matmul(fx_[l], V[l]) + c[l])\n",
    "        L_inv[l] = tf.reduce_mean(0.5*(gx_[l] - (x[l-1] + tf.random_normal(c[l].get_shape(), stddev=noise_inj)))**2,\n",
    "                                  name='L_inv_'+str(l))\n",
    "        train_op_inv[l] = opt.minimize(L_inv[l], var_list=[V[l], c[l]])\n",
    "\n",
    "train_bp = opt.minimize(L_g, var_list=[i for i in W+b if i is not None])\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(tf.nn.softmax(x[-1]), 1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clean up\n",
    "train_op = [i for i in train_op if i is not None]\n",
    "train_op_inv = [i for i in train_op_inv if i is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for l in range(layers):\n",
    "    if L[l] is not None:\n",
    "        tf.scalar_summary('L'+str(l), L[l])\n",
    "    if L_inv[l] is not None:\n",
    "        tf.scalar_summary('L_inv'+str(l), L_inv[l])\n",
    "tf.scalar_summary('L_g', L_g)\n",
    "\n",
    "#tf.scalar_summary('learning rate', train_op._lr_t)\n",
    "#tf.scalar_summary('learning rate2', train_op._lr)\n",
    "#tf.scalar_summary('accuracy', accuracy)\n",
    "\n",
    "merged_summary_op = tf.merge_all_summaries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0000 loss: 2.3013 accuracy: 0.1300\n",
      "iter: 0100 loss: 2.2949 accuracy: 0.1500\n",
      "iter: 0200 loss: 2.2973 accuracy: 0.0800\n",
      "iter: 0300 loss: 2.2902 accuracy: 0.1300\n",
      "iter: 0400 loss: 2.3027 accuracy: 0.1300\n",
      "iter: 0500 loss: 2.3001 accuracy: 0.1100\n",
      "iter: 0600 loss: 2.2909 accuracy: 0.1500\n",
      "iter: 0700 loss: 2.2886 accuracy: 0.1700\n",
      "iter: 0800 loss: 2.3010 accuracy: 0.1000\n",
      "iter: 0900 loss: 2.3045 accuracy: 0.1100\n",
      "iter: 1000 loss: 2.3044 accuracy: 0.1100\n",
      "iter: 1100 loss: 2.3027 accuracy: 0.1000\n",
      "iter: 1200 loss: 2.3024 accuracy: 0.1200\n",
      "iter: 1300 loss: 2.2952 accuracy: 0.1400\n",
      "iter: 1400 loss: 2.3022 accuracy: 0.1000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-161d2caff1d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_bp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mloss_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mL_g\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerged_summary_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0msummary_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jeff/anaconda/lib/python2.7/site-packages/tensorflow/python/training/summary_io.pyc\u001b[0m in \u001b[0;36madd_summary\u001b[0;34m(self, summary, global_step)\u001b[0m\n\u001b[1;32m    135\u001b[0m       \u001b[0msumm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParseFromString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m       \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msumm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     \u001b[0mevent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevent_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEvent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwall_time\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mglobal_step\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m       \u001b[0mevent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jeff/anaconda/lib/python2.7/site-packages/google/protobuf/internal/python_message.pyc\u001b[0m in \u001b[0;36minit\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfield_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield_value\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m       \u001b[0mfield\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_GetFieldByName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_descriptor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m       \u001b[0;32mif\u001b[0m \u001b[0mfield\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    491\u001b[0m         raise TypeError(\"%s() got an unexpected keyword argument '%s'\" %\n\u001b[1;32m    492\u001b[0m                         (message_descriptor.name, field_name))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "run+=1\n",
    "\n",
    "summary_writer = tf.train.SummaryWriter('/tmp/tensorflow_logs/'+str(run), sess.graph)\n",
    "\n",
    "for i in range(10000):\n",
    "    inds_batch, [x_batch, y_batch] = get_rand_batch([x_data, y_data])\n",
    "    feed_dict={x_in: x_batch, y: y_batch, epoch: i}\n",
    "    # first train Linv\n",
    "    #sess.run(train_op, feed_dict=feed_dict)\n",
    "    #sess.run(train_op_inv, feed_dict=feed_dict)\n",
    "    sess.run(train_bp, feed_dict=feed_dict)\n",
    "    loss_val, summary_str, acc_val = sess.run([L_g, merged_summary_op, accuracy], feed_dict=feed_dict)\n",
    "    summary_writer.add_summary(summary_str, i)\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print \"iter:\", \"%04d\" % (i), \\\n",
    "              \"loss:\", \"{:.4f}\".format(loss_val), \\\n",
    "              \"accuracy:\", \"{:.4f}\".format(acc_val)\n",
    "\n",
    "print \"finished\"\n",
    "summary_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
