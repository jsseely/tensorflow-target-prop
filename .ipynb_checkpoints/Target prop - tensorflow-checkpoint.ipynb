{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%reset -f\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, '/Users/jeff/Documents/Python/_projects/tdadl/')\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from toy_data import *\n",
    "from tproptflow import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "iter: 0000 loss: 2.4004 accuracy: 0.1440\n",
      "iter: 0100 loss: 1.3400 accuracy: 0.7560\n",
      "iter: 0200 loss: 1.2373 accuracy: 0.8260\n",
      "iter: 0300 loss: 1.1640 accuracy: 0.8700\n",
      "iter: 0400 loss: 1.1232 accuracy: 0.8940\n",
      "iter: 0500 loss: 1.1004 accuracy: 0.9040\n",
      "iter: 0600 loss: 1.0873 accuracy: 0.9080\n",
      "iter: 0700 loss: 1.0804 accuracy: 0.9180\n",
      "iter: 0800 loss: 1.0772 accuracy: 0.9220\n",
      "iter: 0900 loss: 1.0766 accuracy: 0.9200\n",
      "finished\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "iter: 0000 loss: 2.4004 accuracy: 0.1440\n",
      "iter: 0100 loss: 1.3400 accuracy: 0.7560\n",
      "iter: 0200 loss: 1.2371 accuracy: 0.8280\n",
      "iter: 0300 loss: 1.1639 accuracy: 0.8700\n",
      "iter: 0400 loss: 1.1230 accuracy: 0.8940\n",
      "iter: 0500 loss: 1.1001 accuracy: 0.9040\n",
      "iter: 0600 loss: 1.0869 accuracy: 0.9080\n",
      "iter: 0700 loss: 1.0798 accuracy: 0.9180\n",
      "iter: 0800 loss: 1.0764 accuracy: 0.9220\n",
      "iter: 0900 loss: 1.0756 accuracy: 0.9180\n",
      "finished\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "iter: 0000 loss: 2.4004 accuracy: 0.1440\n",
      "iter: 0100 loss: 1.3392 accuracy: 0.7580\n",
      "iter: 0200 loss: 1.2361 accuracy: 0.8260\n",
      "iter: 0300 loss: 1.1631 accuracy: 0.8720\n",
      "iter: 0400 loss: 1.1216 accuracy: 0.8940\n",
      "iter: 0500 loss: 1.0978 accuracy: 0.9000\n",
      "iter: 0600 loss: 1.0833 accuracy: 0.9080\n",
      "iter: 0700 loss: 1.0747 accuracy: 0.9180\n",
      "iter: 0800 loss: 1.0695 accuracy: 0.9220\n",
      "iter: 0900 loss: 1.0667 accuracy: 0.9220\n",
      "finished\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "iter: 0000 loss: 2.4004 accuracy: 0.1440\n",
      "iter: 0100 loss: 1.3323 accuracy: 0.7620\n",
      "iter: 0200 loss: 1.2251 accuracy: 0.8340\n",
      "iter: 0300 loss: 1.1543 accuracy: 0.8760\n",
      "iter: 0400 loss: 1.1090 accuracy: 0.8960\n",
      "iter: 0500 loss: 1.0797 accuracy: 0.9060\n",
      "iter: 0600 loss: 1.0591 accuracy: 0.9080\n",
      "iter: 0700 loss: 1.0431 accuracy: 0.9140\n",
      "iter: 0800 loss: 1.0296 accuracy: 0.9220\n",
      "iter: 0900 loss: 1.0179 accuracy: 0.9260\n",
      "finished\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "iter: 0000 loss: 2.4004 accuracy: 0.1440\n",
      "iter: 0100 loss: 1.2927 accuracy: 0.7740\n",
      "iter: 0200 loss: 1.1599 accuracy: 0.8520\n",
      "iter: 0300 loss: 1.0957 accuracy: 0.8980\n",
      "iter: 0400 loss: 1.0554 accuracy: 0.9060\n",
      "iter: 0500 loss: 1.0277 accuracy: 0.9140\n",
      "iter: 0600 loss: 1.0075 accuracy: 0.9200\n",
      "iter: 0700 loss: 0.9918 accuracy: 0.9220\n",
      "iter: 0800 loss: 0.9792 accuracy: 0.9240\n",
      "iter: 0900 loss: 0.9691 accuracy: 0.9280\n",
      "finished\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "iter: 0000 loss: 2.4004 accuracy: 0.1440\n",
      "iter: 0100 loss: 1.2798 accuracy: 0.7940\n",
      "iter: 0200 loss: 1.1499 accuracy: 0.8740\n",
      "iter: 0300 loss: 1.0905 accuracy: 0.8960\n",
      "iter: 0400 loss: 1.0534 accuracy: 0.9060\n",
      "iter: 0500 loss: 1.0273 accuracy: 0.9160\n",
      "iter: 0600 loss: 1.0075 accuracy: 0.9200\n",
      "iter: 0700 loss: 0.9922 accuracy: 0.9200\n",
      "iter: 0800 loss: 0.9799 accuracy: 0.9240\n",
      "iter: 0900 loss: 0.9696 accuracy: 0.9280\n",
      "finished\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "iter: 0000 loss: 2.4007 accuracy: 0.1420\n",
      "iter: 0100 loss: 1.2897 accuracy: 0.7880\n",
      "iter: 0200 loss: 1.1567 accuracy: 0.8740\n",
      "iter: 0300 loss: 1.0971 accuracy: 0.8960\n",
      "iter: 0400 loss: 1.0591 accuracy: 0.9060\n",
      "iter: 0500 loss: 1.0325 accuracy: 0.9180\n",
      "iter: 0600 loss: 1.0122 accuracy: 0.9200\n",
      "iter: 0700 loss: 0.9964 accuracy: 0.9240\n",
      "iter: 0800 loss: 0.9838 accuracy: 0.9260\n",
      "iter: 0900 loss: 0.9734 accuracy: 0.9280\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "for i in [0, 10, 100, 1000, 10000, 100000, 1000000]:\n",
    "    run_tprop(beta=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# debuggin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from toy_data import *\n",
    "sys.path.insert(0, '/Users/jeff/Documents/Python/_projects/tdadl/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "  ## Get data from toy_data.py\n",
    "  data = mnist_data()\n",
    "  #data = xor_data()\n",
    "  data.outputs = data.outputs[:,:]\n",
    "  data.inputs = data.inputs[:,:2]\n",
    "\n",
    "  ## Model parameters\n",
    "  m_dim = data.inputs.shape[1]\n",
    "  p_dim = data.outputs.shape[1]\n",
    "\n",
    "  # Layer naming convention\n",
    "  # Layer 0: input vector space\n",
    "  # Layer P: output vector space (same dimension as data.output)\n",
    "  # Convention: \"layers = P\", so that len(l_dim)=P+1\n",
    "\n",
    "  layers = 8\n",
    "  l_dim = [m_dim] + (layers-1)*[240] + [p_dim]\n",
    "  stddev = 0.05 # noise for weight inits\n",
    "  b_init = 0.0 # init for bias terms\n",
    "  alpha = 0.5 # x_tar[-1] = x[-1] - alpha*(dL/dx[-1])\n",
    "\n",
    "  batch_size = 10\n",
    "\n",
    "  ## Model\n",
    "  tf.reset_default_graph()\n",
    "\n",
    "  # placeholders\n",
    "  x_in = tf.placeholder(tf.float32, shape=[batch_size, m_dim]) # Input\n",
    "  y = tf.placeholder(tf.float32, shape=[batch_size, p_dim]) # Output\n",
    "  epoch = tf.placeholder(tf.float32, shape=None) # training iteration\n",
    "\n",
    "  noise_inj = .1/(1.+epoch/200.) # stddev\n",
    "\n",
    "  # Initialize lists.\n",
    "  b = (layers+1)*[None] \n",
    "  W = (layers+1)*[None]\n",
    "  x = (layers+1)*[None]\n",
    "\n",
    "  L = (layers+1)*[None] # Local layer loss for training W and b\n",
    "  L_inv = (layers+1)*[None] # Local inverse loss for training V and c\n",
    "  C = (layers+1)*[None] # Local target loss for training x_target\n",
    "\n",
    "  x_ = (layers+1)*[None] # targets\n",
    "  V = (layers+1)*[None] # feedback matrix\n",
    "  c = (layers+1)*[None] # feedback bias\n",
    "\n",
    "  x_c = (layers+1)*[None] # x + noise\n",
    "  fx_c = (layers+1)*[None] # f(x + noise)\n",
    "\n",
    "  train_op_inv = (layers+1)*[None]\n",
    "  train_op = (layers+1)*[None]\n",
    "  train_op_C = (layers+1)*[None]\n",
    "\n",
    "  def f(ll, zz):\n",
    "    \"\"\"map from layer ll-1 to ll\"\"\"\n",
    "    return tf.nn.sigmoid(tf.matmul(zz, W[ll]) + b[ll], name='f')\n",
    "  def f_stop(ll, zz):\n",
    "    \"\"\"like f, but with stop_gradients on parameters\"\"\"\n",
    "    return tf.nn.sigmoid(tf.matmul(zz, tf.stop_gradient(W[ll]) + tf.stop_gradient(b[ll])), name='f_stop')\n",
    "\n",
    "  def g(ll, zz):\n",
    "    \"\"\"map from layer ll to ll-1\"\"\"\n",
    "    return tf.nn.sigmoid(tf.matmul(zz, V[ll]) + c[ll], name='g')\n",
    "\n",
    "  # Forward graph\n",
    "  x[0] = x_in\n",
    "  for l in range(1, layers+1):\n",
    "    with tf.name_scope('Layer_Forward'+str(l)):\n",
    "      b[l] = tf.Variable(tf.constant(b_init, shape=[1, l_dim[l]]), name='b')\n",
    "      #W[l] = tf.Variable(np.linalg.qr(np.random.randn(l_dim[l-1], l_dim[l]))[0].astype('float32'), name='W') #orthonormal initialization\n",
    "      W[l] = tf.Variable(tf.truncated_normal([l_dim[l-1], l_dim[l]], stddev=np.sqrt(6./(l_dim[l-1]+l_dim[l]))), name='W') # Random initialization\n",
    "      x[l] = f(l, x[l-1])\n",
    "\n",
    "  # Top layer loss / top layer target\n",
    "  L[-1] = tf.reduce_mean(-tf.reduce_sum(y*tf.log(tf.nn.softmax(x[-1])), reduction_indices=[1]))\n",
    "  x_[-1] = x[-1] - alpha*tf.gradients(L[-1], [x[-1]])[0]\n",
    "\n",
    "  # OPTION 1 Feedback graph\n",
    "  # for l in range(layers, 1, -1):\n",
    "  #   with tf.name_scope('Layer_Feedback'+str(l)):\n",
    "  #     c[l] = tf.Variable(tf.constant(b_init, shape=[1, l_dim[l-1]]), name='c')\n",
    "  #     V[l] = tf.Variable(tf.truncated_normal([l_dim[l], l_dim[l-1]], stddev=np.sqrt(6./(l_dim[l-1]+l_dim[l]))), name='V')\n",
    "  #     x_[l-1] = x[l-1] - g(l, x[l]) + g(l, x_[l])\n",
    "\n",
    "  # OPTION 2 Specify/optimize targets exactly.\n",
    "  for l in range(layers-1, 0, -1):\n",
    "    with tf.name_scope('Layer_targets'+str(l)):\n",
    "      x_[l] = tf.Variable(tf.constant(0., shape=[batch_size, l_dim[l]]), name='x_target')\n",
    "\n",
    "  # # Corrupted targets\n",
    "  # for l in range(1, layers):\n",
    "  #   x_c[l] = tf.stop_gradient(tf.random_normal([1, l_dim[l]], mean=x[l], stddev=noise_inj), name='x_c')\n",
    "  #   fx_c[l+1] = tf.stop_gradient(f(l+1, x_c[l]), name='fx_c')\n",
    "\n",
    "  # Loss functions\n",
    "  # OPTION 1\n",
    "  for l in range(1, layers):\n",
    "    L[l] = tf.reduce_mean(0.5*(f(l, tf.stop_gradient(x[l-1])) - tf.stop_gradient(x_[l]))**2, name='L')\n",
    "  # OPTION 2\n",
    "  beta = 0\n",
    "  for l in range(1, layers):\n",
    "    C[l] = tf.reduce_mean(0.5*(tf.stop_gradient(x_[l+1]) - f_stop(l+1, x_[l]))**2) + beta*tf.reduce_mean(0.5*(tf.stop_gradient(x[l]) - x_[l])**2)\n",
    "  # for i in range(2, layers+1):\n",
    "  #   L_inv[i] = tf.reduce_mean(0.5*(g(i, fx_c[i]) - x_c[i-1])**2, name='L_inv')\n",
    "\n",
    "  # Optimizers\n",
    "  opt = tf.train.AdamOptimizer(0.001)\n",
    "  for l in range(1, layers+1):\n",
    "    train_op[l] = tf.train.GradientDescentOptimizer(0.1).minimize(L[l], var_list=[W[l], b[l]])\n",
    "  for l in range(1, layers): \n",
    "    train_op_C[l] = tf.train.GradientDescentOptimizer(0.1).minimize(C[l], var_list=[x_[l]])\n",
    "  # for l in range(2, layers+1):\n",
    "  #   train_op_inv[l] = opt.minimize(L_inv[l], var_list=[V[l], c[l]])\n",
    "\n",
    "  # Backprop. for reference\n",
    "  train_bp = opt.minimize(L[-1], var_list=[i for i in W+b if i is not None])\n",
    "\n",
    "  correct_prediction = tf.equal(tf.argmax(tf.nn.softmax(x[-1]), 1), tf.argmax(y,1))\n",
    "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "  # clean up\n",
    "  train_op = [i for i in train_op if i is not None]\n",
    "  #train_op_inv = [i for i in train_op_inv if i is not None]\n",
    "  train_op_C = [i for i in train_op_C if i is not None]\n",
    "\n",
    "  # Tensorboard\n",
    "  for l in range(layers+1):\n",
    "    if L[l] is not None:\n",
    "      tf.scalar_summary('L'+str(l), L[l])\n",
    "    if L_inv[l] is not None:\n",
    "      tf.scalar_summary('L_inv'+str(l), L_inv[l])\n",
    "  tf.scalar_summary('accuracy', accuracy)\n",
    "\n",
    "  for var in tf.all_variables():\n",
    "    tf.histogram_summary(var.name, var)\n",
    "  merged_summary_op = tf.merge_all_summaries()\n",
    "\n",
    "  #TODO: get prev run from directory/\n",
    "  run=1\n",
    "\n",
    "  sess = tf.Session()\n",
    "  sess.run(tf.initialize_all_variables())\n",
    "\n",
    "  run+=1\n",
    "\n",
    "  summary_writer = tf.train.SummaryWriter('/tmp/targ-prop/'+str(run), sess.graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = mnist_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inds = data.outputs[:,:3].sum(axis=1).astype('bool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print data.inputs[inds].shape\n",
    "print data.outputs[inds].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(inds[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_batch, y_batch = data.inputs[:batch_size], data.outputs[:batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grads, varbs = opt.compute_gradients(L[-1], var_list=[W[-1], b[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "varbs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b[1].get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.gradients(v3, [v1, v2, v3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.gradients(L[5], [b[5]])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_[l+1].get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f_stop(l+1, x_[l]).get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x[l].get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_[l].get_shape()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
