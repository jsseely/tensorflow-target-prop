{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reset -f\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, '/Users/jeff/Documents/Python/_projects/tdadl/')\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from toy_data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# seaborn setup\n",
    "cmap = sns.color_palette('Set1')\n",
    "sns.set_palette(cmap)\n",
    "\n",
    "data = mnist_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Model parameters\n",
    "m_dim = data.inputs.shape[1]\n",
    "p_dim = data.outputs.shape[1]\n",
    "\n",
    "#for...\n",
    "#  layer 0 input vector space\n",
    "#  layer 1:7 intermediate\n",
    "#  layer 8 final layer\n",
    "#convention: \"layers = 8\"\n",
    "\n",
    "layers = 8\n",
    "l_dim = [m_dim] + (layers-1)*[240] + [p_dim]\n",
    "stddev = 0.01\n",
    "b_init = 0.0\n",
    "alpha = 1\n",
    "\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Model\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# placeholders\n",
    "x_in = tf.placeholder(tf.float32, shape=[batch_size, m_dim]) # Input\n",
    "y = tf.placeholder(tf.float32, shape=[batch_size, p_dim]) # Output\n",
    "epoch = tf.placeholder(tf.float32, shape=None) # training iteration\n",
    "\n",
    "noise_inj = .1/(1.+epoch/200.) # stddev\n",
    "\n",
    "# Initialize lists.\n",
    "b = (layers+1)*[None] \n",
    "W = (layers+1)*[None]\n",
    "x = (layers+1)*[None]\n",
    "\n",
    "L = (layers+1)*[None]\n",
    "L_inv = (layers+1)*[None]\n",
    "\n",
    "x_ = (layers+1)*[None]\n",
    "V = (layers+1)*[None]\n",
    "c = (layers+1)*[None]\n",
    "\n",
    "x_c = (layers+1)*[None]\n",
    "fx_c = (layers+1)*[None]\n",
    "\n",
    "train_op_inv = (layers+1)*[None]\n",
    "train_op = (layers+1)*[None]\n",
    "\n",
    "def f(ll, zz):\n",
    "    \"\"\"map from layer ll-1 to ll\"\"\"\n",
    "    return tf.nn.sigmoid(tf.matmul(zz, W[ll]) + b[ll], name='f')\n",
    "\n",
    "def g(ll, zz):\n",
    "    \"\"\"map from layer ll to ll-1\"\"\"\n",
    "    return tf.nn.sigmoid(tf.matmul(zz, V[ll]) + c[ll], name='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Forward graph\n",
    "x[0] = x_in\n",
    "for l in range(1, layers+1):\n",
    "    with tf.name_scope('Layer_Forward'+str(l)):\n",
    "        b[l] = tf.Variable(tf.constant(b_init, shape=[1, l_dim[l]]), name='b')\n",
    "        W[l] = tf.Variable(tf.truncated_normal([l_dim[l-1], l_dim[l]], stddev=np.sqrt(6./(l_dim[l-1]+l_dim[l]))), name='W')\n",
    "        x[l] = f(l, x[l-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Top layer loss / top layer target\n",
    "L[-1] = tf.reduce_mean(-tf.reduce_sum(y*tf.log(tf.nn.softmax(x[-1])), reduction_indices=[1]))\n",
    "x_[-1] = x[-1] - alpha*tf.gradients(L[-1], [x[-1]])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Feedback graph\n",
    "for l in range(layers, 1, -1):\n",
    "    with tf.name_scope('Layer_Feedback'+str(l)):\n",
    "        c[l] = tf.Variable(tf.constant(b_init, shape=[1, l_dim[l-1]]), name='c')\n",
    "        V[l] = tf.Variable(tf.truncated_normal([l_dim[l], l_dim[l-1]], stddev=np.sqrt(6./(l_dim[l-1]+l_dim[l]))), name='V')\n",
    "        x_[l-1] = x[l-1] - g(l, x[l]) + g(l, x_[l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Corrupted targets\n",
    "for l in range(1, layers):\n",
    "    x_c[l] = tf.stop_gradient(tf.random_normal([1, l_dim[l]], mean=x[l], stddev=noise_inj), name='x_c')\n",
    "    fx_c[l+1] = tf.stop_gradient(f(l+1, x_c[l]), name='fx_c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loss functions\n",
    "for l in range(1, layers):\n",
    "    L[l] = tf.reduce_mean(0.5*(f(l, tf.stop_gradient(x[l-1])) - tf.stop_gradient(x_[l]))**2, name='L')\n",
    "for i in range(2, layers+1):\n",
    "    L_inv[i] = tf.reduce_mean(0.5*(g(i, fx_c[i]) - x_c[i-1])**2, name='L_inv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "opt = tf.train.AdamOptimizer(0.001)\n",
    "for l in range(1, layers+1):\n",
    "    train_op[l] = opt.minimize(L[l], var_list=[W[l], b[l]])\n",
    "for l in range(2, layers+1):\n",
    "    train_op_inv[l] = opt.minimize(L_inv[l], var_list=[V[l], c[l]])\n",
    "\n",
    "# Backprop. for reference\n",
    "train_bp = opt.minimize(L[-1], var_list=[i for i in W+b if i is not None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(tf.nn.softmax(x[-1]), 1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clean up\n",
    "train_op = [i for i in train_op if i is not None]\n",
    "train_op_inv = [i for i in train_op_inv if i is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tensorboard\n",
    "for l in range(layers+1):\n",
    "    if L[l] is not None:\n",
    "        tf.scalar_summary('L'+str(l), L[l])\n",
    "    if L_inv[l] is not None:\n",
    "        tf.scalar_summary('L_inv'+str(l), L_inv[l])\n",
    "tf.scalar_summary('accuracy', accuracy)\n",
    "\n",
    "for var in tf.all_variables():\n",
    "    tf.histogram_summary(var.name, var)\n",
    "merged_summary_op = tf.merge_all_summaries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0000 loss: 2.3148 accuracy: 0.1700\n",
      "iter: 0100 loss: 2.3106 accuracy: 0.0900\n",
      "iter: 0200 loss: 2.3006 accuracy: 0.1100\n",
      "iter: 0300 loss: 2.3022 accuracy: 0.0800\n",
      "iter: 0400 loss: 2.2959 accuracy: 0.1600\n",
      "iter: 0500 loss: 2.3042 accuracy: 0.1000\n",
      "iter: 0600 loss: 2.3114 accuracy: 0.0500\n",
      "iter: 0700 loss: 2.3024 accuracy: 0.1300\n",
      "iter: 0800 loss: 2.3047 accuracy: 0.0900\n",
      "iter: 0900 loss: 2.2974 accuracy: 0.1600\n",
      "iter: 1000 loss: 2.3017 accuracy: 0.1100\n",
      "iter: 1100 loss: 2.3032 accuracy: 0.1200\n",
      "iter: 1200 loss: 2.3005 accuracy: 0.1300\n",
      "iter: 1300 loss: 2.3100 accuracy: 0.0600\n",
      "iter: 1400 loss: 2.3024 accuracy: 0.1100\n",
      "iter: 1500 loss: 2.2994 accuracy: 0.1100\n",
      "iter: 1600 loss: 2.3025 accuracy: 0.1000\n",
      "iter: 1700 loss: 2.2984 accuracy: 0.1400\n",
      "iter: 1800 loss: 2.3040 accuracy: 0.0900\n",
      "iter: 1900 loss: 2.2984 accuracy: 0.1300\n",
      "iter: 2000 loss: 2.2989 accuracy: 0.1300\n",
      "iter: 2100 loss: 2.2975 accuracy: 0.1300\n",
      "iter: 2200 loss: 2.3030 accuracy: 0.1200\n",
      "iter: 2300 loss: 2.2994 accuracy: 0.1700\n",
      "iter: 2400 loss: 2.3035 accuracy: 0.0900\n",
      "iter: 2500 loss: 2.3133 accuracy: 0.0600\n",
      "iter: 2600 loss: 2.3029 accuracy: 0.0900\n",
      "iter: 2700 loss: 2.3005 accuracy: 0.1300\n",
      "iter: 2800 loss: 2.3015 accuracy: 0.1100\n",
      "iter: 2900 loss: 2.2983 accuracy: 0.1400\n",
      "iter: 3000 loss: 2.2962 accuracy: 0.1400\n",
      "iter: 3100 loss: 2.3015 accuracy: 0.1200\n",
      "iter: 3200 loss: 2.3009 accuracy: 0.1200\n",
      "iter: 3300 loss: 2.3021 accuracy: 0.0800\n",
      "iter: 3400 loss: 2.3029 accuracy: 0.0900\n",
      "iter: 3500 loss: 2.3003 accuracy: 0.1100\n",
      "iter: 3600 loss: 2.3011 accuracy: 0.1100\n",
      "iter: 3700 loss: 2.3058 accuracy: 0.1000\n",
      "iter: 3800 loss: 2.3044 accuracy: 0.1000\n",
      "iter: 3900 loss: 2.3019 accuracy: 0.1200\n",
      "iter: 4000 loss: 2.2928 accuracy: 0.1600\n",
      "iter: 4100 loss: 2.2969 accuracy: 0.1600\n",
      "iter: 4200 loss: 2.2959 accuracy: 0.1400\n",
      "iter: 4300 loss: 2.3075 accuracy: 0.0600\n",
      "iter: 4400 loss: 2.3016 accuracy: 0.1500\n",
      "iter: 4500 loss: 2.3058 accuracy: 0.0800\n",
      "iter: 4600 loss: 2.2953 accuracy: 0.1400\n",
      "iter: 4700 loss: 2.2966 accuracy: 0.1500\n",
      "iter: 4800 loss: 2.3018 accuracy: 0.1100\n",
      "iter: 4900 loss: 2.3046 accuracy: 0.1200\n",
      "iter: 5000 loss: 2.1600 accuracy: 0.1500\n",
      "iter: 5100 loss: 2.1678 accuracy: 0.1400\n",
      "iter: 5200 loss: 2.0886 accuracy: 0.1800\n",
      "iter: 5300 loss: 2.0521 accuracy: 0.1600\n",
      "iter: 5400 loss: 2.0579 accuracy: 0.1700\n",
      "iter: 5500 loss: 2.0863 accuracy: 0.2000\n",
      "iter: 5600 loss: 2.1151 accuracy: 0.1400\n",
      "iter: 5700 loss: 2.0348 accuracy: 0.2100\n",
      "iter: 5800 loss: 2.0432 accuracy: 0.2000\n",
      "iter: 5900 loss: 2.0117 accuracy: 0.1700\n",
      "iter: 6000 loss: 1.9887 accuracy: 0.2200\n",
      "iter: 6100 loss: 2.0141 accuracy: 0.1800\n",
      "iter: 6200 loss: 1.9697 accuracy: 0.2000\n",
      "iter: 6300 loss: 2.0062 accuracy: 0.1600\n",
      "iter: 6400 loss: 2.0191 accuracy: 0.1600\n",
      "iter: 6500 loss: 2.0126 accuracy: 0.1600\n",
      "iter: 6600 loss: 2.0174 accuracy: 0.1900\n",
      "iter: 6700 loss: 2.0391 accuracy: 0.1900\n",
      "iter: 6800 loss: 2.0143 accuracy: 0.1800\n",
      "iter: 6900 loss: 2.0074 accuracy: 0.2100\n",
      "iter: 7000 loss: 2.0023 accuracy: 0.1700\n",
      "iter: 7100 loss: 1.9956 accuracy: 0.2100\n",
      "iter: 7200 loss: 2.0981 accuracy: 0.1500\n",
      "iter: 7300 loss: 2.0745 accuracy: 0.1200\n",
      "iter: 7400 loss: 1.9792 accuracy: 0.2000\n",
      "iter: 7500 loss: 2.0110 accuracy: 0.1800\n",
      "iter: 7600 loss: 2.0262 accuracy: 0.1700\n",
      "iter: 7700 loss: 1.9852 accuracy: 0.1500\n",
      "iter: 7800 loss: 2.0613 accuracy: 0.1800\n",
      "iter: 7900 loss: 2.0627 accuracy: 0.1600\n",
      "iter: 8000 loss: 2.0739 accuracy: 0.1500\n",
      "iter: 8100 loss: 2.0390 accuracy: 0.2100\n",
      "iter: 8200 loss: 2.0360 accuracy: 0.2400\n",
      "iter: 8300 loss: 2.0450 accuracy: 0.1700\n",
      "iter: 8400 loss: 2.0350 accuracy: 0.1800\n",
      "iter: 8500 loss: 1.9959 accuracy: 0.1900\n",
      "iter: 8600 loss: 2.0230 accuracy: 0.2300\n",
      "iter: 8700 loss: 2.0162 accuracy: 0.1600\n",
      "iter: 8800 loss: 1.9839 accuracy: 0.2300\n",
      "iter: 8900 loss: 1.9916 accuracy: 0.2000\n",
      "iter: 9000 loss: 1.9795 accuracy: 0.2100\n",
      "iter: 9100 loss: 1.9747 accuracy: 0.1600\n",
      "iter: 9200 loss: 1.9206 accuracy: 0.2500\n",
      "iter: 9300 loss: 1.9525 accuracy: 0.1500\n",
      "iter: 9400 loss: 1.9318 accuracy: 0.2700\n",
      "iter: 9500 loss: 1.9094 accuracy: 0.1800\n",
      "iter: 9600 loss: 1.9377 accuracy: 0.3000\n",
      "iter: 9700 loss: 1.9077 accuracy: 0.3400\n",
      "iter: 9800 loss: 1.8328 accuracy: 0.4100\n",
      "iter: 9900 loss: 1.7974 accuracy: 0.3800\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "run+=1\n",
    "\n",
    "summary_writer = tf.train.SummaryWriter('/tmp/targ-prop/'+str(run), sess.graph)\n",
    "\n",
    "for i in range(100000):\n",
    "    x_batch, y_batch = data.rand_batch(batch_size)\n",
    "    feed_dict={x_in: x_batch, y: y_batch, epoch: i}\n",
    "    sess.run(train_op_inv, feed_dict=feed_dict)\n",
    "    sess.run(train_op, feed_dict=feed_dict)\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        loss_val, summary_str, acc_val = sess.run([L[-1], merged_summary_op, accuracy], feed_dict=feed_dict)\n",
    "        summary_writer.add_summary(summary_str, i)\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print \"iter:\", \"%04d\" % (i), \\\n",
    "              \"loss:\", \"{:.4f}\".format(loss_val), \\\n",
    "              \"accuracy:\", \"{:.4f}\".format(acc_val)\n",
    "\n",
    "print \"finished\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "   \n",
    "# # Global loss\n",
    "# #L_g = tf.reduce_mean(0.5*(x[-1] - y)**2, name=\"global_loss\")\n",
    "# with tf.variable_scope('global_loss'):\n",
    "#     L_g = tf.reduce_mean(-tf.reduce_sum(y*tf.log(tf.nn.softmax(x[-1])), reduction_indices=[1]))\n",
    "\n",
    "# # Top-layer targets\n",
    "# L_grads = tf.gradients(L_g, [x[-1]])\n",
    "\n",
    "# x_tar[-1] = x[-1] - alpha*L_grads[0]\n",
    "# #x_tar[-2] = x[-2] - alpha*L_grads[1]\n",
    "\n",
    "# # Target graph\n",
    "# for l in range(layers,0,-1): # from M to 2\n",
    "#     with tf.name_scope('layer_target'+str(l)) as scope:\n",
    "#         if x_tar[l] is None:\n",
    "#             x_tar[l] = x[l] + gx_tar[l+1] - gx[l+1] # gx[l+1] must be defined of course...\n",
    "#         if l > 1:\n",
    "#             c[l] = tf.Variable(tf.constant(b_init, shape=[1, l_dim[l-1]]), name='c')\n",
    "#             V[l] = tf.Variable(tf.truncated_normal([l_dim[l], l_dim[l-1]], stddev=np.sqrt(6./(l_dim[l-1]+l_dim[l]))), name='V')\n",
    "#             gx[l] = tf.nn.sigmoid(tf.add(tf.matmul(x[l], V[l]), c[l]), name='g_x')\n",
    "\n",
    "#             gx_tar[l] = tf.nn.sigmoid(tf.add(tf.matmul(x_tar[l], V[l]), c[l]), name='g_x_tar')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Optimizers\n",
    "# opt = tf.train.AdamOptimizer(0.001)\n",
    "\n",
    "# train_op_inv = (layers+1)*[None]\n",
    "# train_op = (layers+1)*[None]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# fx_ = (layers+1)*[None] # f(x + eps)\n",
    "# gx_ = (layers+1)*[None] # g(f(x + eps))\n",
    "\n",
    "# for l in range(2, layers+1):\n",
    "#     with tf.name_scope('L_inv_layer'+str(l)) as scope:\n",
    "#         fx_[l] = tf.nn.sigmoid(tf.matmul(x[l-1], W[l]) + b[l] + tf.random_normal(b[l].get_shape(), stddev=noise_inj))\n",
    "#         gx_[l] = tf.nn.sigmoid(tf.matmul(fx_[l], V[l]) + c[l])\n",
    "#         with tf.name_scope('L_inv'):\n",
    "#             L_inv[l] = tf.reduce_mean(0.5*(gx_[l] - (x[l-1] + tf.random_normal(c[l].get_shape(), stddev=noise_inj)))**2,\n",
    "#                                       name='L_inv')\n",
    "#         train_op_inv[l] = opt.minimize(L_inv[l], var_list=[V[l], c[l]])\n",
    "\n",
    "# for l in range(1, layers+1):\n",
    "#     with tf.name_scope('L_layer'+str(l)) as scope:\n",
    "#         with tf.name_scope('L'):\n",
    "#             L[l] = tf.reduce_mean(0.5*(x[l] - x_tar[l])**2, name=\"L\")\n",
    "#         train_op[l] = opt.minimize(L[l], var_list=[W[l], b[l]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
