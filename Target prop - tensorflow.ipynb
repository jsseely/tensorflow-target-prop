{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reset -f\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, '/Users/jeff/Documents/Python/_projects/tdadl/')\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from toy_data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# seaborn setup\n",
    "cmap = sns.color_palette('Set1')\n",
    "sns.set_palette(cmap)\n",
    "\n",
    "data = mnist_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Model parameters\n",
    "m_dim = data.inputs.shape[1]\n",
    "p_dim = data.outputs.shape[1]\n",
    "\n",
    "#for...\n",
    "#  layer 0 input vector space\n",
    "#  layer 1:7 intermediate\n",
    "#  layer 8 final layer\n",
    "#convention: \"layers = 8\"\n",
    "\n",
    "layers = 8\n",
    "l_dim = [m_dim] + (layers-1)*[240] + [p_dim]\n",
    "stddev = 0.01\n",
    "b_init = 0.0\n",
    "alpha = 1\n",
    "\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Model\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# placeholders\n",
    "x_in = tf.placeholder(tf.float32, shape=[batch_size, m_dim]) # Input\n",
    "y = tf.placeholder(tf.float32, shape=[batch_size, p_dim]) # Output\n",
    "epoch = tf.placeholder(tf.float32, shape=None) # training iteration\n",
    "\n",
    "noise_inj = .1/(1.+epoch/200.) # stddev\n",
    "\n",
    "# Initialize lists.\n",
    "b = (layers+1)*[None] \n",
    "W = (layers+1)*[None]\n",
    "x = (layers+1)*[None]\n",
    "\n",
    "L = (layers+1)*[None]\n",
    "L_inv = (layers+1)*[None]\n",
    "\n",
    "x_ = (layers+1)*[None]\n",
    "V = (layers+1)*[None]\n",
    "c = (layers+1)*[None]\n",
    "\n",
    "x_c = (layers+1)*[None]\n",
    "fx_c = (layers+1)*[None]\n",
    "\n",
    "train_op_inv = (layers+1)*[None]\n",
    "train_op = (layers+1)*[None]\n",
    "\n",
    "def f(ll, zz):\n",
    "    \"\"\"map from layer ll-1 to ll\"\"\"\n",
    "    return tf.nn.sigmoid(tf.matmul(zz, W[ll]) + b[ll], name='f')\n",
    "\n",
    "def g(ll, zz):\n",
    "    \"\"\"map from layer ll to ll-1\"\"\"\n",
    "    return tf.nn.sigmoid(tf.matmul(zz, V[ll]) + c[ll], name='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Forward graph\n",
    "x[0] = x_in\n",
    "for l in range(1, layers+1):\n",
    "    with tf.name_scope('Layer_Forward'+str(l)):\n",
    "        b[l] = tf.Variable(tf.constant(b_init, shape=[1, l_dim[l]]), name='b')\n",
    "        W[l] = tf.Variable(tf.truncated_normal([l_dim[l-1], l_dim[l]], stddev=np.sqrt(6./(l_dim[l-1]+l_dim[l]))), name='W')\n",
    "        x[l] = f(l, x[l-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Top layer loss / top layer target\n",
    "L[-1] = tf.reduce_mean(-tf.reduce_sum(y*tf.log(tf.nn.softmax(x[-1])), reduction_indices=[1]))\n",
    "x_[-1] = x[-1] - alpha*tf.gradients(L[-1], [x[-1]])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Feedback graph\n",
    "for l in range(layers, 1, -1):\n",
    "    with tf.name_scope('Layer_Feedback'+str(l)):\n",
    "        c[l] = tf.Variable(tf.constant(b_init, shape=[1, l_dim[l-1]]), name='c')\n",
    "        V[l] = tf.Variable(tf.truncated_normal([l_dim[l], l_dim[l-1]], stddev=np.sqrt(6./(l_dim[l-1]+l_dim[l]))), name='V')\n",
    "        x_[l-1] = x[l-1] - g(l, x[l]) + g(l, x_[l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Corrupted targets\n",
    "for l in range(1, layers):\n",
    "    x_c[l] = tf.stop_gradient(tf.random_normal([1, l_dim[l]], mean=x[l], stddev=noise_inj), name='x_c')\n",
    "    fx_c[l+1] = tf.stop_gradient(f(l+1, x_c[l]), name='fx_c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loss functions\n",
    "for l in range(1, layers):\n",
    "    L[l] = tf.reduce_mean(0.5*(f(l, tf.stop_gradient(x[l-1])) - tf.stop_gradient(x_[l]))**2, name='L')\n",
    "for i in range(2, layers+1):\n",
    "    L_inv[i] = tf.reduce_mean(0.5*(g(i, fx_c[i]) - x_c[i-1])**2, name='L_inv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "opt = tf.train.AdamOptimizer(0.001)\n",
    "for l in range(1, layers+1):\n",
    "    train_op[l] = opt.minimize(L[l], var_list=[W[l], b[l]])\n",
    "for l in range(2, layers+1):\n",
    "    train_op_inv[l] = opt.minimize(L_inv[l], var_list=[V[l], c[l]])\n",
    "\n",
    "# Backprop. for reference\n",
    "train_bp = opt.minimize(L[-1], var_list=[i for i in W+b if i is not None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(tf.nn.softmax(x[-1]), 1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clean up\n",
    "train_op = [i for i in train_op if i is not None]\n",
    "train_op_inv = [i for i in train_op_inv if i is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tensorboard\n",
    "for l in range(layers+1):\n",
    "    if L[l] is not None:\n",
    "        tf.scalar_summary('L'+str(l), L[l])\n",
    "    if L_inv[l] is not None:\n",
    "        tf.scalar_summary('L_inv'+str(l), L_inv[l])\n",
    "tf.scalar_summary('accuracy', accuracy)\n",
    "\n",
    "for var in tf.all_variables():\n",
    "    tf.histogram_summary(var.name, var)\n",
    "merged_summary_op = tf.merge_all_summaries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "run+=1\n",
    "\n",
    "summary_writer = tf.train.SummaryWriter('/tmp/targ-prop/'+str(run), sess.graph)\n",
    "\n",
    "for i in range(100000):\n",
    "    x_batch, y_batch = data.rand_batch(batch_size)\n",
    "    feed_dict={x_in: x_batch, y: y_batch, epoch: i}\n",
    "    sess.run(train_op_inv, feed_dict=feed_dict)\n",
    "    sess.run(train_op, feed_dict=feed_dict)\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        loss_val, summary_str, acc_val = sess.run([L[-1], merged_summary_op, accuracy], feed_dict=feed_dict)\n",
    "        summary_writer.add_summary(summary_str, i)\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print \"iter:\", \"%04d\" % (i), \\\n",
    "              \"loss:\", \"{:.4f}\".format(loss_val), \\\n",
    "              \"accuracy:\", \"{:.4f}\".format(acc_val)\n",
    "\n",
    "print \"finished\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "   \n",
    "# # Global loss\n",
    "# #L_g = tf.reduce_mean(0.5*(x[-1] - y)**2, name=\"global_loss\")\n",
    "# with tf.variable_scope('global_loss'):\n",
    "#     L_g = tf.reduce_mean(-tf.reduce_sum(y*tf.log(tf.nn.softmax(x[-1])), reduction_indices=[1]))\n",
    "\n",
    "# # Top-layer targets\n",
    "# L_grads = tf.gradients(L_g, [x[-1]])\n",
    "\n",
    "# x_tar[-1] = x[-1] - alpha*L_grads[0]\n",
    "# #x_tar[-2] = x[-2] - alpha*L_grads[1]\n",
    "\n",
    "# # Target graph\n",
    "# for l in range(layers,0,-1): # from M to 2\n",
    "#     with tf.name_scope('layer_target'+str(l)) as scope:\n",
    "#         if x_tar[l] is None:\n",
    "#             x_tar[l] = x[l] + gx_tar[l+1] - gx[l+1] # gx[l+1] must be defined of course...\n",
    "#         if l > 1:\n",
    "#             c[l] = tf.Variable(tf.constant(b_init, shape=[1, l_dim[l-1]]), name='c')\n",
    "#             V[l] = tf.Variable(tf.truncated_normal([l_dim[l], l_dim[l-1]], stddev=np.sqrt(6./(l_dim[l-1]+l_dim[l]))), name='V')\n",
    "#             gx[l] = tf.nn.sigmoid(tf.add(tf.matmul(x[l], V[l]), c[l]), name='g_x')\n",
    "\n",
    "#             gx_tar[l] = tf.nn.sigmoid(tf.add(tf.matmul(x_tar[l], V[l]), c[l]), name='g_x_tar')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Optimizers\n",
    "# opt = tf.train.AdamOptimizer(0.001)\n",
    "\n",
    "# train_op_inv = (layers+1)*[None]\n",
    "# train_op = (layers+1)*[None]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# fx_ = (layers+1)*[None] # f(x + eps)\n",
    "# gx_ = (layers+1)*[None] # g(f(x + eps))\n",
    "\n",
    "# for l in range(2, layers+1):\n",
    "#     with tf.name_scope('L_inv_layer'+str(l)) as scope:\n",
    "#         fx_[l] = tf.nn.sigmoid(tf.matmul(x[l-1], W[l]) + b[l] + tf.random_normal(b[l].get_shape(), stddev=noise_inj))\n",
    "#         gx_[l] = tf.nn.sigmoid(tf.matmul(fx_[l], V[l]) + c[l])\n",
    "#         with tf.name_scope('L_inv'):\n",
    "#             L_inv[l] = tf.reduce_mean(0.5*(gx_[l] - (x[l-1] + tf.random_normal(c[l].get_shape(), stddev=noise_inj)))**2,\n",
    "#                                       name='L_inv')\n",
    "#         train_op_inv[l] = opt.minimize(L_inv[l], var_list=[V[l], c[l]])\n",
    "\n",
    "# for l in range(1, layers+1):\n",
    "#     with tf.name_scope('L_layer'+str(l)) as scope:\n",
    "#         with tf.name_scope('L'):\n",
    "#             L[l] = tf.reduce_mean(0.5*(x[l] - x_tar[l])**2, name=\"L\")\n",
    "#         train_op[l] = opt.minimize(L[l], var_list=[W[l], b[l]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
